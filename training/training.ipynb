{"cells":[{"cell_type":"markdown","source":["# Training of the U-net to impliment the unflooding \n"],"metadata":{}},{"cell_type":"markdown","source":["## Loading the modules "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","\n","import matplotlib.pyplot as plt\n","import matplotlib as mlp\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler,MaxAbsScaler, RobustScaler\n","from sklearn.model_selection import train_test_split\n","from skimage.transform import resize\n","import numpy as np\n","from prep_data import Dataset\n","import torch\n","from torch import autograd\n","from torch.utils.data import DataLoader\n","from unet import UNet\n","import torch.nn as  nn\n","import random\n","from scipy.ndimage.filters import gaussian_filter1d\n","from util import plot_models1D, plot_models1D, plot_history, plot_models1D2, plot_r2, r2_score\n","from noise_layer import GaussianNoise\n","import time \n","\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Define the training function "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","\n","\n","\n","def run_training(model,opt,criterion,training_data,valid_data,num_epoch=100,batchsz=32):\n","\tntrain = training_data.x_data.shape[0]\n","\tnvalid = valid_data.y_data.shape[0]\n","\ttrain_loader= DataLoader (dataset=training_data,batch_size=batchsz,shuffle=True)\n","\tvalid_loader= DataLoader(dataset=valid_data,batch_size=batchsz,shuffle=True)\n","\tloss_train = []\n","\tloss_valid = []\n","\tR2_train = []\n","\tR2_valid = []\n","\tt_start = time.time()\n","\tadd_noise = GaussianNoise(0.01) # This is a noise layer, used for regularization only for training, changing the standard deviation might change the output\n","\tfor epoch in range (num_epoch):\n","\t\tepoch_train_loss=0\n","\t\tR2_train_running=0\n","\t\t# loop over batches\n","\t\tfor batch,data in enumerate (train_loader,0):\n","\t\t\tinputs,targets = data\n","\t\t\t# inputs = inputs.view(inputs.shape[0],1,inputs.shape[1])\n","\t\t\ttargets = targets.view(targets.shape[0],1,targets.shape[1])\n","\t\t\tinputs = add_noise(inputs)\n","\t\t\tpred = model(inputs)\n","\t\t\tloss = criterion(pred,targets) \n","\t\t\tR2_train_running += r2_score(targets,pred)\n","\t\t\tepoch_train_loss += loss.item()\n","\t\t\tloss.backward()\n","\t\t\toptimizer.step()\n","\t\t\toptimizer.zero_grad()\n","\t\tepoch_train_loss = epoch_train_loss/(ntrain/batchsz)\n","\t\tR2_train_running = R2_train_running/(ntrain/batchsz) \n","\n","\t\twith torch.no_grad():\n","\t\t\tepoch_valid_loss=0 \n","\t\t\tR2_valid_running=0\n","\t\t\tfor batch,data in enumerate (valid_loader,0):\n","\t\t\t\tinputs,targets = data\n","\t\t\t\ttargets = targets.view(targets.shape[0],1,targets.shape[1])\n","\t\t\t\tpred = model(inputs)\n","\t\t\t\tloss = criterion(pred,targets)\n","\t\t\t\tR2_valid_running += r2_score(targets,pred)\n","\t\t\t\tepoch_valid_loss += loss.item()\n","\t\t\tepoch_valid_loss = epoch_valid_loss/(nvalid/batchsz)\n","\t\t\tR2_valid_running = R2_valid_running/(nvalid/batchsz) \n","\n","\t\tloss_train.append(epoch_train_loss)\n","\t\tloss_valid.append(epoch_valid_loss)\t\n","\t\tR2_train.append(R2_train_running)\n","\t\tR2_valid.append(R2_valid_running)\n","\t\tprint(f'''epoch: {epoch+1:3}/{num_epoch:3}  Training_loss: {epoch_train_loss:.5e}  Validation_loss: {epoch_valid_loss:.5e}\n","\t\t R2_Training: {R2_train_running:.5}  R2_Validation: {R2_valid_running:.5}''')\n","\tt_end = time.time()\n","\tprint(\"=================================================\")\n","\tprint(f\"Training time is {(t_end-t_start)/60} minutes.\")\n","\tprint(\"=================================================\")\n","\n","\treturn model,np.array(loss_train),np.array(loss_valid),np.array(R2_train),np.array(R2_valid)\n","\n","\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Define training hyperparameter"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["batchsz= 32\n","LR = 0.001\n","num_epoch =  100\n","chanl = 2\n","feat=16 \n","\n","netname = 'unet'  \n","\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Reading the data "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","\t# reading the inputs and targets\n","path = '<data path>' \n","\n","ifile = 1  # initial file index\n","endfile= 8000 # final file index\n","\n","# Read the first file \n","inp2=np.load(path+'inv_m'+str(ifile)+'.npy')   # inverted models \n","oup2 =np.load(path+'true_m'+str(ifile)+'.npy') # true models\n","init2 =np.load(path+'init_m'+str(ifile)+'.npy') # initial models used for FWI\n","print('shape of the first reading file ',init2.shape)\n","\n","# Allocate the arrays \n","nm = inp2.shape[0]\n","nt = inp2.shape[1]\n","inp = np.zeros((endfile,nt))\n","oup = np.zeros((endfile,nt))\n","init = np.zeros((endfile,nt))\n","print('number of allocating array  for data is ',init.shape)\n","inp[0,:] = inp2\n","oup[0,:] = oup2\n","init[0,:] = init2\n","for k in range(ifile,endfile):\n","\tinp_tmp = np.load(path+'inv_m'+str(k)+'.npy')\n","\toup_tmp = np.load(path+'true_m'+str(k)+'.npy')\n","\tinit_tmp = np.load(path+'init_m'+str(k)+'.npy')\n","\tinp[nm,:] = inp_tmp\n","\toup[nm,:] = oup_tmp\n","\tinit[nm,:] = init_tmp\n","\tnm +=1\n","\tif k%1000==0:print('number of models', nm)       \n","\t\n","print ('loaded shapes for input and output',inp.shape, oup.shape)\t\n","path = './output/'  \n","plot_models1D2(inp,oup,init,inp.shape[0],5,5)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Reshape the data and concatenate to create the input for the network "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["inp = inp.reshape((inp.shape[0],1,inp.shape[1]))\n","init = init.reshape((init.shape[0],1,init.shape[1]))\n","inp = np.concatenate((inp,init),axis=1)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Normalize the data bt the salt velocity \n","inp = inp/4.5\n","oup = oup/4.5\n","init = init/4.5\n","\n","\n","# split training and validation\n","x_train,x_valid, y_train, y_valid = train_test_split(inp,oup, test_size=0.2)\n","ntrain = x_train.shape[0]\n","nvalid = x_valid.shape[0]\n","\n","\n","np.save(path+'NNmodel/xtrain_%s'%netname,x_train)\n","np.save(path+'NNmodel/ytrain_%s'%netname,y_train)\n","np.save(path+'NNmodel/xvalid_%s'%netname,x_valid)\n","np.save(path+'NNmodel/yvalid_%s'%netname,y_valid)\n","\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Prepare data for pytorch, define the network"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["print('shape of the training data is ', x_train.shape)\n","print('number of training: ',ntrain )\n","print('number of validation: ',nvalid )\n","print('Batch: ',batchsz )\n","\n","# prepare data for pytorch loader\n","training_data=Dataset(x_train,y_train)\n","valid_data =Dataset(x_valid,y_valid)\n","\n","\n","# define NN model\n","model = UNet(in_channels=chanl,out_channels=1,init_features=feat)\n","# model = UNet(in_channels=10,out_channels=1,init_features=chanl) # For this use unference 2\n","model.cuda()\n","# this should be outer loop\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.MSELoss() \n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Run training "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","# Run trainnig \n","model, loss_train, loss_valid, r2_train,r2_valid = run_training(model,optimizer,criterion,training_data,valid_data,num_epoch)\n","\n","\n","print('Done training yaaaaaay --------')\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Save model, losses, "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["\n","torch.save(model.state_dict(),path+'NNmodel/'+netname)\n","\n","np.save(path+'NNmodel/Training_loss'+netname,np.array(loss_train))     \n","np.save(path+'NNmodel/Validation_loss'+netname,np.array(loss_valid))\n","np.save(path+'NNmodel/Training_R2'+netname,np.array(r2_train))\n","np.save(path+'NNmodel/Validation_R2'+netname,np.array(r2_valid))\n","\n","\n","plot_history(loss_train,loss_valid,netname) \n","plot_history(r2_train,r2_valid,netname) \n","\n"],"outputs":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}